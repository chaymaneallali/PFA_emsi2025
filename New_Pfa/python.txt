this is the all python models ( #!/usr/bin/env python
# coding: utf-8

# In[ ]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display


# In[2]:


import sys
sys.path.append('../model')
from data_processing import load_and_clean_data, fetch_posters
from recommendation_engine import RecommendationEngine


# In[3]:


movies_df = pd.read_csv('../data/movies.csv')
ratings_df = pd.read_csv('../data/ratings.csv')
links_df = pd.read_csv('../data/links.csv')


# In[4]:


print("Movies Data:")
display(movies_df.head(2))
print("\nRatings Data:")
display(ratings_df.head(2))


# In[5]:


# Data preprocessing
movies_data = load_and_clean_data()
movies_data = fetch_posters(movies_data)


# In[6]:


genre_counts = pd.Series([genre for genres in movies_data['genres'] for genre in genres]).value_counts()

plt.figure(figsize=(12,6))
sns.barplot(x=genre_counts.values, y=genre_counts.index, palette='viridis')
plt.title('Movie Genre Distribution')
plt.xlabel('Count')
plt.ylabel('Genre')
plt.show()


# In[7]:


# Ratings analysis
plt.figure(figsize=(10,6))
sns.countplot(x='rating', data=ratings_df, palette='coolwarm')
plt.title('User Rating Distribution')
plt.show()


# In[8]:


engine = RecommendationEngine(movies_data, ratings_df)
engine.initialize_models()


# In[9]:


# Content-based filtering test
def test_content_recommendations(movie_title):
    results = engine.search_by_title(movie_title)
    print(f"Recommendations for '{movie_title}':")
    display(results[['title', 'genres', 'poster_path']])

test_content_recommendations("Toy Story")


# In[10]:


# Collaborative filtering test
def test_collaborative_recommendations(user_id):
    results = engine.get_collaborative_recommendations(user_id)
    print(f"Recommendations for user {user_id}:")
    display(results[['title', 'genres', 'poster_path', 'avg_rating']])

test_collaborative_recommendations(1)


# In[27]:


# Hybrid recommendations test
def test_hybrid_recommendations(user_input):
    results = engine.hybrid_recommendation(user_input)
    print(f"Hybrid recommendations based on '{user_input}':")
    display(results[['title', 'genres', 'poster_path', 'score']])

test_hybrid_recommendations("Matrix")


# In[12]:


# Model persistence test
engine.save_models("../model/recommendation_models.pkl")


# In[13]:


# Verify model loading
new_engine = RecommendationEngine(movies_data, ratings_df)
new_engine.load_models("../model/recommendation_models.pkl")


# In[14]:


get_ipython().system('jupyter nbconvert --to script model.ipynb --output-dir ../model')


# In[19]:


import requests
import json


# In[20]:


def get_tmdb_id(imdb_id):
    try:
        response = requests.get(
            f"https://api.themoviedb.org/3/find/{imdb_id}",
            params={
                "api_key": "5c98496341763fd19721ae7b3afc2a5c",
                "external_source": "imdb_id"
            }
        )
        if response.status_code == 200:
            return response.json()['movie_results'][0]['id']
        return None
    except Exception as e:
        print(f"Error fetching TMDB ID for IMDb {imdb_id}: {str(e)}")
        return None

# In your data processing:
def load_and_clean_data():
    movies_df = pd.read_csv('../data/movies.csv')
    links_df = pd.read_csv('../data/links.csv')

    # Convert IMDb IDs to proper format
    links_df['imdbId'] = links_df['imdbId'].apply(lambda x: f"tt{x:07d}")

    # Fetch missing TMDB IDs
    missing_tmdb = links_df[links_df['tmdbId'].isna()]
    for idx, row in missing_tmdb.iterrows():
        tmdb_id = get_tmdb_id(row['imdbId'])
        if tmdb_id:
            links_df.at[idx, 'tmdbId'] = tmdb_id

    # Merge datasets
    movies_data = movies_df.merge(
        links_df[['movieId', 'tmdbId']], 
        on='movieId', 
        how='left'
    )

    # Rest of your processing...
    return movies_data


# In[21]:


test_tmdb_id = 862  # TMDB ID for Toy Story
response = requests.get(
    f"https://api.themoviedb.org/3/movie/{test_tmdb_id}",
    params={"api_key": "5c98496341763fd19721ae7b3afc2a5c"}
)
print("API Response:", response.status_code)  # Should be 200
print("Poster Path:", response.json().get('poster_path'))  # Should show a path


# In[22]:


try:
    with open('poster_cache.json', 'r') as f:
        cache = json.load(f)
    print(f"Found {len(cache)} cached entries")
except FileNotFoundError:
    print("No cache file found - it will be created during processing")
except Exception as e:
    print(f"Error loading cache: {str(e)}")


# In[25]:


# Final verification after fixes
movies_data = load_and_clean_data()
movies_data = fetch_posters(movies_data)

print("\nPoster coverage statistics:")
print(f"Total movies: {len(movies_data)}")
print(f"Movies with posters: {movies_data['poster_path'].notna().sum()}")
print(f"Coverage: {movies_data['poster_path'].notna().mean():.1%}")

print("\nSample movies with posters:")
display(movies_data[movies_data['poster_path'].notna()][['title', 'poster_path']].sample(5))


# In[26]:


# Check cache hit rate
cached_ids = set(movies_data['tmdbId'].astype(str))
cache_hits = len([id for id in cached_ids if str(id) in cache])
print(f"Cache hits: {cache_hits}/{len(cached_ids)}")


# In[ ]:



  ) (import pandas as pd
import re
import json
import os
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
TMDB_API_KEY = "5c98496341763fd19721ae7b3afc2a5c"

def load_and_clean_data():
    movies_df = pd.read_csv(DATA_DIR / "movies.csv")
    links_df = pd.read_csv(DATA_DIR / "links.csv")
    
    movies_data = movies_df.merge(
        links_df[['movieId', 'tmdbId', 'imdbId']], 
        on='movieId', 
        how='left'
    )

    movies_data['movieId'] = movies_data['movieId'].astype(int)


    
    movies_data['title'] = movies_data['title'].apply(
        lambda x: re.sub("[^a-zA-Z0-9 ]", "", x)
    )
    
    movies_data['genres'] = movies_data['genres'].str.split('|')
    movies_data = movies_data[~movies_data['genres'].apply(
        lambda x: '(no genres listed)' in x
    )]
    
    movies_data['tmdbId'] = pd.to_numeric(movies_data['tmdbId'], errors='coerce').astype('Int64')
    movies_data['imdbId'] = movies_data['imdbId'].apply(lambda x: f"tt{x:07d}")
    
    return movies_data

def fetch_posters(movies_data):
    CACHE_FILE = BASE_DIR / "model/poster_cache.json"
    
    def fetch_poster(tmdb_id):
        if pd.isna(tmdb_id) or tmdb_id == 0:
            return None
        try:
            response = requests.get(
                f'https://api.themoviedb.org/3/movie/{tmdb_id}',
                params={'api_key': TMDB_API_KEY},
                timeout=10
            )
            if response.status_code == 200:
                return response.json().get('poster_path')
            return None
        except Exception as e:
            print(f"Error fetching {tmdb_id}: {str(e)}")
            return None

    cache = {}
    if CACHE_FILE.exists():
        with open(CACHE_FILE, 'r') as f:
            cache = json.load(f)
    
    tmdb_ids = movies_data['tmdbId'].dropna().unique().astype(int)
    tmdb_ids_str = [str(id) for id in tmdb_ids]
    new_ids = [id for id in tmdb_ids_str if id not in cache]
    
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(fetch_poster, int(id)): id for id in new_ids if id.isdigit()}
        for future in as_completed(futures):
            movie_id = futures[future]
            try:
                cache[movie_id] = future.result()
            except:
                pass
    
    with open(CACHE_FILE, 'w') as f:
        json.dump(cache, f)
    
    movies_data['poster_path'] = movies_data['tmdbId'].apply(
        lambda x: f"https://image.tmdb.org/t/p/w500{cache.get(str(int(x)), None)}" 
        if pd.notnull(x) else None
    )
    
    return movies_data ) ( import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pickle

class RecommendationEngine:
    def __init__(self, movies_data, ratings_df):
        self.movies_data = movies_data
        self.ratings_df = ratings_df
        self.vectorizer_title = None
        self.vectorizer_genres = None
        self.tfidf_title = None
        self.tfidf_genres = None

    def initialize_models(self):
        # Title vectorizer
        self.vectorizer_title = TfidfVectorizer(ngram_range=(1, 2))
        self.tfidf_title = self.vectorizer_title.fit_transform(self.movies_data['title'])
        
        # Genres vectorizer
        self.movies_data['genres_text'] = self.movies_data['genres'].apply(lambda x: ' '.join(x))
        self.vectorizer_genres = TfidfVectorizer(ngram_range=(1, 2))
        self.tfidf_genres = self.vectorizer_genres.fit_transform(self.movies_data['genres_text'])

    def save_models(self, path="models.pkl"):
        with open(path, "wb") as f:
            pickle.dump({
                "vectorizer_title": self.vectorizer_title,
                "tfidf_title": self.tfidf_title,
                "vectorizer_genres": self.vectorizer_genres,
                "tfidf_genres": self.tfidf_genres
            }, f)

            

    def load_models(self, path="models.pkl"):
        with open(path, "rb") as f:
            models = pickle.load(f)
            self.vectorizer_title = models["vectorizer_title"]
            self.tfidf_title = models["tfidf_title"]
            self.vectorizer_genres = models["vectorizer_genres"]
            self.tfidf_genres = models["tfidf_genres"]


    def load_models(self, path="model/recommendation_models.pkl"):
        with open(path, "rb") as f:
            models = pickle.load(f)
            self.vectorizer_title = models["vectorizer_title"]
            self.tfidf_title = models["tfidf_title"]
            self.vectorizer_genres = models["vectorizer_genres"]
            self.tfidf_genres = models["tfidf_genres"]

    def search_by_title(self, title_query, top_n=10):
        tfidf_query = self.vectorizer_title.transform([title_query])
        similarities = cosine_similarity(tfidf_query, self.tfidf_title).flatten()
        top_indices = similarities.argsort()[::-1][:top_n]

        results = self.movies_data.iloc[top_indices].copy()
        results['similarity'] = similarities[top_indices]
        return results

    def get_collaborative_recommendations(self, user_id, top_n=10):
        user_ratings = self.ratings_df[self.ratings_df['userId'] == user_id]
        top_rated = user_ratings[user_ratings['rating'] >= 4.0]
        top_movies = top_rated.merge(self.movies_data, on='movieId')

        avg_ratings = self.ratings_df.groupby('movieId')['rating'].mean().reset_index()

        avg_ratings.columns = ['movieId', 'avg_rating']

        top_movies = top_movies.merge(avg_ratings, on='movieId', how='left')

        return top_movies.sort_values(by='rating', ascending=False).head(top_n)
    
    
    

    def hybrid_recommendation(self, input_query, top_n=10):
        content_recs = self.search_by_title(input_query, top_n=30)
        top_content_ids = content_recs['movieId'].values

        avg_ratings = self.ratings_df.groupby('movieId')['rating'].mean().reset_index()
        avg_ratings.columns = ['movieId', 'avg_rating']

        merged = pd.merge(content_recs, avg_ratings, on='movieId', how='left')
        merged['score'] = 0.7 * merged['similarity'] + 0.3 * (merged['avg_rating'] / 5)
        return merged.sort_values(by='score', ascending=False).head(top_n)
    

    def get_top_rated_movies(self, min_ratings=100):
        rating_counts = self.ratings_df.groupby('movieId')['rating'] \
            .agg(num_ratings=('count'), avg_rating=('mean')) \
            .reset_index()\
            .rename(columns={'count': 'num_ratings', 'mean': 'avg_rating'})
    
    # Filter movies with minimum ratings
        filtered = rating_counts[rating_counts['num_ratings'] >= min_ratings]
    
    # Merge with movie data
        return filtered.merge(
            self.movies_data,
            on='movieId',
            how='left'
        ).sort_values('avg_rating', ascending=False)


    def get_content_recommendations(self, movie_id, top_n=10):
        movie_idx = self.movies_data[self.movies_data['movieId'] == movie_id].index[0]
    
        title_sim = cosine_similarity(
            self.tfidf_title[movie_idx], 
            self.tfidf_title
        ).flatten()
    
        genre_sim = cosine_similarity(
            self.tfidf_genres[movie_idx], 
            self.tfidf_genres
        ).flatten()
    
        combined_sim = 0.6 * title_sim + 0.4 * genre_sim
    
        similar_indices = combined_sim.argsort()[::-1][1:top_n+1]
        return self.movies_data.iloc[similar_indices]

    
    def get_hybrid_recommendations(self, user_id, top_n=10):
        try:
        # Collaborative filtering part
            user_ratings = self.ratings_df[self.ratings_df['userId'] == user_id]
        
            if user_ratings.empty:
                top_rated = self.get_top_rated_movies(10)  # Use lower min_ratings
                return top_rated.head(top_n)
        
        # Get user's top movies (handle cases with <3 ratings)
            user_top_movies = user_ratings.nlargest(3, 'rating', keep='all')['movieId'].tolist()
        
        # Content-based recommendations
            content_recs = []
            for movie_id in user_top_movies:
                try:
                    recs = self.get_content_recommendations(movie_id, top_n)
                    content_recs.append(recs)
                except IndexError:
                    continue
        
            if not content_recs:
                return self.get_top_rated_movies(10).head(top_n)
            
            combined = pd.concat(content_recs)
        
        # Handle missing data
            combined = combined.merge(
                self.ratings_df.groupby('movieId')['rating']
                    .agg(avg_rating=('mean'), num_ratings=('count'))
                    .reset_index(),
                on='movieId',
                how='left'
            )
        
        # Fill missing values
            combined['avg_rating'] = combined['avg_rating'].fillna(2.5)
            combined['num_ratings'] = combined['num_ratings'].fillna(0)
        
        # Calculate score
            combined['score'] = (
                0.5 * combined['avg_rating'] +
                0.3 * combined['num_ratings'] / combined['num_ratings'].max() +
                0.2 * combined['similarity']
            )
        
            return combined.sort_values('score', ascending=False).head(top_n)
    
        except Exception as e:
            print(f"Hybrid error: {str(e)}")
            return self.get_top_rated_movies(10).head(top_n) ) ( from flask import Flask, jsonify, request
from flask_cors import CORS
import pandas as pd
import pickle
import sys
from pathlib import Path




# Add root directory to Python path
sys.path.append(str(Path(__file__).parent.parent))
# Replace relative imports
from model.data_processing import load_and_clean_data, fetch_posters
from model.recommendation_engine import RecommendationEngine
import logging

# Initialize Flask app
app = Flask(__name__)

CORS(app, resources={
    r"/api/*": {
        "origins": ["http://localhost:8090", "http://localhost:4200"],
        "allow_headers": ["*"],
        "methods": ["GET", "POST"]
    }
})


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global engine instance
engine = None

def initialize_api():
    global engine
    try:
        movies_data = load_and_clean_data()
        ratings_df = pd.read_csv("data/ratings.csv")
        
        if ratings_df.empty or movies_data.empty:
            raise ValueError("Movies or ratings data is empty!")
        
        movies_data = fetch_posters(movies_data)
        
        engine = RecommendationEngine(movies_data, ratings_df)
        
        model_path = str(Path(__file__).parent.parent / "model/recommendation_models.pkl")
        engine.load_models(model_path)
        
        logger.info("API initialization completed successfully")
    except Exception as e:
        logger.error(f"Initialization failed: {str(e)}")
        raise

@app.route('/api/health')
def health_check():
    return jsonify({"status": "healthy", "model_loaded": engine is not None})

@app.route('/api/recommendations/top-rated', methods=['GET'])
def get_top_rated():
    try:
        min_ratings = request.args.get('min_ratings', default=100, type=int)
        results = engine.get_top_rated_movies(min_ratings)
        return jsonify(results.to_dict(orient='records'))
    except Exception as e:
        logger.error(f"Top rated error: {str(e)}", exc_info=True)  # Add exc_info
        return jsonify({"error": "Recommendation failed", "detail": str(e)}), 500

@app.route('/api/recommendations/hybrid/<int:user_id>', methods=['GET'])
def get_hybrid_recommendations(user_id):
    try:
        top_n = request.args.get('top_n', default=10, type=int)
        results = engine.get_hybrid_recommendations(user_id, top_n)
        
        # Convert NaN and handle genres
        results = results.where(pd.notnull(results), None)
        results['genres'] = results['genres'].apply(
            lambda x: x if isinstance(x, list) else []
        )
        
        return jsonify(results.to_dict(orient='records'))
    
    except Exception as e:
        logger.error(f"Hybrid error: {str(e)}", exc_info=True)
        return jsonify({
            "error": "Recommendation failed",
            "detail": "Using fallback recommendations",
            "fallback": engine.get_top_rated_movies(10).head(5).to_dict(orient='records')
        }), 200

@app.route('/api/recommendations/collaborative/<int:user_id>', methods=['GET'])
def get_collaborative_recommendations(user_id):
    try:
        top_n = request.args.get('top_n', default=10, type=int)
        results = engine.get_collaborative_recommendations(user_id, top_n)
        return jsonify(results.to_dict(orient='records'))
    except Exception as e:
        logger.error(f"Collaborative error for user {user_id}: {str(e)}")
        return jsonify({"error": "Recommendation failed"}), 500

@app.route('/api/movies/<int:movie_id>', methods=['GET'])
def get_movie_details(movie_id):
    try:
        # Convert to string for comparison if needed
        movie = engine.movies_data[engine.movies_data['movieId'].astype(int) == movie_id]
        
        if movie.empty:
            return jsonify({"error": f"Movie {movie_id} not found"}), 404
            
        recommendations = engine.get_content_recommendations(movie_id)
        
        return jsonify({
            "movie": movie.iloc[0].to_dict(),
            "recommendations": recommendations.to_dict(orient='records')
        })
    except Exception as e:
        logger.error(f"Movie details error for {movie_id}: {str(e)}", exc_info=True)
        return jsonify({"error": "Movie lookup failed", "detail": str(e)}), 500

@app.route('/api/movies', methods=['GET'])
def get_all_movies():
    try:
        page = request.args.get('page', default=1, type=int)
        per_page = request.args.get('per_page', default=100, type=int)
        
        start_idx = (page - 1) * per_page
        end_idx = start_idx + per_page
        
        results = engine.movies_data.iloc[start_idx:end_idx]
        return jsonify({
            "page": page,
            "per_page": per_page,
            "total_movies": len(engine.movies_data),
            "results": results.to_dict(orient='records')
        })
    except Exception as e:
        logger.error(f"Movies list error: {str(e)}")
        return jsonify({"error": "Movie list failed"}), 500

    
if __name__ == '__main__':
    initialize_api()
    app.run(host='0.0.0.0', port=5000, threaded=True) ) dont response me i will send you my spring boot in the next message 